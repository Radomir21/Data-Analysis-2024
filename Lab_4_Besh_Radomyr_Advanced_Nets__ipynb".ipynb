{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Radomir21/Data-Analysis-2024/blob/main/Lab_4_Besh_Radomyr_Advanced_Nets__ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.  Завдання щодо генерації текстів або машинного перекладу (на вибір) на базі рекурентних мереж або трансформерів (на вибір). Вирішіть завдання щодо генерації текстів або машинного перекладу. Особливо вітаються україномовні моделі.**"
      ],
      "metadata": {
        "id": "M0B3jJ9pGRxu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vw2-qybKWDE7"
      },
      "outputs": [],
      "source": [
        "#імпорти бібліотек\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization, MultiHeadAttention, LayerNormalization, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFb2II-REzyh",
        "outputId": "65940811-d1a6-4204-87af-2da6b474072a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = '/content/drive/MyDrive/Data Analyzis/ukr.txt'"
      ],
      "metadata": {
        "id": "fpY1aDrfW_Dj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, ukr, _ = line.strip().split(\"\\t\")\n",
        "    ukr = \"[start] \" + ukr + \" [end]\"\n",
        "    text_pairs.append((eng, ukr))"
      ],
      "metadata": {
        "id": "L8uG2XObXE4O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOJjAAqlXxP1",
        "outputId": "974d57b2-9d82-40eb-aa69-69b05924fa47"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"Who's that guy, anyway?\", '[start] Хто цей чувак взагалі такий? [end]')\n",
            "(\"Don't look for us.\", '[start] Не шукай нас. [end]')\n",
            "('Mr. Jackson is our principal.', '[start] Пан Джексон — наш директор. [end]')\n",
            "(\"I'll do whatever it takes to get you back into my life.\", '[start] Я зроблю все необхідне, щоб ти повернулася в моє життя. [end]')\n",
            "('Tom is looking for an apartment.', '[start] Том шукає квартиру. [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
        "\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edCg3tFEX5rM",
        "outputId": "528115a3-04f1-4204-87ad-1f0a3212615f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "159432 total pairs\n",
            "111604 training pairs\n",
            "23914 validation pairs\n",
            "23914 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "eng_vectorization = TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length)\n",
        "ukr_vectorization = TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length + 1, standardize=custom_standardization)\n",
        "\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_ukr_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "ukr_vectorization.adapt(train_ukr_texts)"
      ],
      "metadata": {
        "id": "gZsTv6ExYCNx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_eng_texts[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA8PMj7NFI17",
        "outputId": "00c4c220-6888-4b29-8b28-7c619b6fbc7c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I wonder how much we'll have to pay.\",\n",
              " 'I have mixed emotions about this.',\n",
              " 'I have a few ideas.',\n",
              " 'Eat a lot of vegetables.',\n",
              " 'You should wear a coat.',\n",
              " 'She lives in the village.',\n",
              " \"Tom's uncle sent him a Christmas card.\",\n",
              " \"You've seen worse.\",\n",
              " 'I got very hungry.',\n",
              " 'I went to the park to play tennis.',\n",
              " 'I taught Tom French three years ago.',\n",
              " 'I did everything I could do.',\n",
              " 'Are you thirsty?',\n",
              " 'Come sit next to me.',\n",
              " \"Don't vote for it.\",\n",
              " 'I know that Tom will try to do that.',\n",
              " 'My pen is new.',\n",
              " 'Who do you like the most?',\n",
              " \"Go home, Tom. You're drunk.\",\n",
              " 'I think that we need to help Tom.',\n",
              " \"I've already paid you.\",\n",
              " 'Tom wants to talk to you again.',\n",
              " 'Everyone loves football.',\n",
              " \"I'm very disappointed in you.\",\n",
              " 'Tom and Mary are nice.',\n",
              " \"We're at the bank.\",\n",
              " 'I want to be able to speak French.',\n",
              " 'Tom sells carpets.',\n",
              " 'You know what I think.',\n",
              " 'That apple is big.',\n",
              " 'How was Tom killed?',\n",
              " 'Why did Tom decide to study French?',\n",
              " 'Tom is a hypocrite.',\n",
              " 'Where should we go?',\n",
              " \"Let's get to the point.\",\n",
              " \"What're Tom and Mary planning?\",\n",
              " 'My name was at the top of the list.',\n",
              " 'Tom said that he thought Mary was mad at him.',\n",
              " 'Have you read this book already?',\n",
              " \"Tom's house is near the beach.\",\n",
              " 'Who gave you this phone?',\n",
              " 'Who does that anymore?',\n",
              " \"It's for Tom.\",\n",
              " 'Count on me.',\n",
              " 'I went downtown this afternoon.',\n",
              " 'What else do you plan on doing?',\n",
              " 'I just want to say you did a good job.',\n",
              " 'Tom is unfaithful.',\n",
              " 'It was a good idea.',\n",
              " 'She felt like crying.',\n",
              " 'Tom shot himself in the leg.',\n",
              " \"Aren't you going to vote?\",\n",
              " 'What an idiot!',\n",
              " 'What newspaper do you subscribe to?',\n",
              " 'Tom ate something.',\n",
              " \"That's very generous of you.\",\n",
              " \"I suspect that Tom isn't afraid to do that.\",\n",
              " 'I was poor.',\n",
              " \"He's just an ordinary student.\",\n",
              " 'I want you to call me as soon as possible.',\n",
              " \"It's been ten years since we last met.\",\n",
              " \"Tom won't go to Boston tomorrow.\",\n",
              " 'Do you think Tom would try to do that?',\n",
              " 'Tom is a genius.',\n",
              " 'Tom is perverse.',\n",
              " 'Would you like some salad?',\n",
              " \"I'm the one who introduced Tom to his wife.\",\n",
              " \"I'll ask Tom to wait.\",\n",
              " 'He was sharpening a knife.',\n",
              " 'I always get up at 6:30.',\n",
              " 'When do we go?',\n",
              " 'You are to stay here till they return.',\n",
              " 'Where did Tom put the keys?',\n",
              " 'You have to trust someone.',\n",
              " \"What's Tom trying to do?\",\n",
              " 'What do you plan to do this summer?',\n",
              " 'By the way, where do you live?',\n",
              " \"We're not always right.\",\n",
              " 'The idea of visiting a church makes me uncomfortable.',\n",
              " 'Tom is at home with his parents.',\n",
              " \"What's the time?\",\n",
              " \"What's your favorite band?\",\n",
              " \"Tom's thirsty.\",\n",
              " 'I think he is angry.',\n",
              " \"We've broken off relations with them.\",\n",
              " 'Tom thought that Mary was listening.',\n",
              " \"I can't believe people like this garbage.\",\n",
              " \"What's your shoe size?\",\n",
              " \"It's small.\",\n",
              " \"You've found it.\",\n",
              " 'Did you know Tom died in Boston?',\n",
              " 'Tom may stay.',\n",
              " 'Pleased to meet you, Tom.',\n",
              " 'How much money do you have?',\n",
              " 'Sing along.',\n",
              " 'Everyone was talking at once.',\n",
              " \"She inherited her mother's blue eyes.\",\n",
              " 'Their dreams came true.',\n",
              " 'Have you ever seen a whale?',\n",
              " \"It's the last chance.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ukr_texts[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZZX1dNaFTsL",
        "outputId": "0828adc1-af7c-4824-abda-92e46b3ded95"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[start] Цікаво, скільки ми муситимо заплатити. [end]',\n",
              " '[start] У мене змішані почуття щодо цього. [end]',\n",
              " '[start] Я маю кілька ідей. [end]',\n",
              " '[start] Їж багато овочів. [end]',\n",
              " '[start] Ти повинен вдягнути плащ. [end]',\n",
              " '[start] Вона живе в селі. [end]',\n",
              " '[start] Дядько Тома відправив йому різдвяну листівку. [end]',\n",
              " '[start] Ви бачили й гірше. [end]',\n",
              " '[start] Я дуже зголодніла. [end]',\n",
              " '[start] Я пішов до парку пограти в теніс. [end]',\n",
              " '[start] Я викладав Тому французьку три роки тому. [end]',\n",
              " '[start] Я зробила все, що могла. [end]',\n",
              " '[start] Ти хочеш пити? [end]',\n",
              " '[start] Йди сядь поруч зі мною. [end]',\n",
              " '[start] Не голосуйте за це. [end]',\n",
              " '[start] Я знаю, що Том спробує це зробити. [end]',\n",
              " '[start] Моя ручка нова. [end]',\n",
              " '[start] Хто тобі подобається більше за всіх? [end]',\n",
              " \"[start] Йди додому, Томе. Ти п'яний. [end]\",\n",
              " '[start] Гадаю, нам потрібно допомогти Тому. [end]',\n",
              " '[start] Я тобі вже заплатила. [end]',\n",
              " '[start] Том знову хоче поговорити з тобою. [end]',\n",
              " '[start] Усі люблять футбол. [end]',\n",
              " '[start] Я в тобі дуже розчарована. [end]',\n",
              " '[start] Том та Мері милі. [end]',\n",
              " '[start] Ми в банку. [end]',\n",
              " '[start] Я хочу вміти говорити французькою. [end]',\n",
              " '[start] Том займається продажем килимів. [end]',\n",
              " '[start] Ви знаєте, що я думаю. [end]',\n",
              " '[start] Це яблуко велике. [end]',\n",
              " '[start] Як Том був убитий? [end]',\n",
              " '[start] Чому Том вирішив вивчати французьку? [end]',\n",
              " '[start] Том — лицемір. [end]',\n",
              " '[start] Куди ми маємо піти? [end]',\n",
              " '[start] Ближче до справи. [end]',\n",
              " '[start] Що Том та Мері планують? [end]',\n",
              " \"[start] Моє ім'я було нагорі списку. [end]\",\n",
              " '[start] Том сказав, що, на його думку, Мері на нього сердита. [end]',\n",
              " '[start] Ви вже читали цю книжку? [end]',\n",
              " '[start] Будинок Тома знаходиться біля пляжу. [end]',\n",
              " '[start] Хто вам дав цей телефон? [end]',\n",
              " '[start] Хто це ще робить? [end]',\n",
              " '[start] Це для Тома. [end]',\n",
              " '[start] Можете на мене розраховувати. [end]',\n",
              " '[start] Сьогодні вдень я ходила до центру міста. [end]',\n",
              " '[start] Чим іще ви плануєте зайнятися? [end]',\n",
              " '[start] Я лише хочу сказати, що ви добре справилися зі своєю роботою. [end]',\n",
              " '[start] Том невірний. [end]',\n",
              " '[start] Це була добра ідея. [end]',\n",
              " '[start] Їй хотілося розплакатися. [end]',\n",
              " '[start] Том вистрілив собі у ногу. [end]',\n",
              " '[start] Хіба ви не будете голосувати? [end]',\n",
              " '[start] Яка ідіотка! [end]',\n",
              " '[start] Яку ви передплачуєте газету? [end]',\n",
              " '[start] Том щось їв. [end]',\n",
              " '[start] Це дуже великодушно з вашого боку. [end]',\n",
              " '[start] Підозрюю, що Том не боїться цим займатися. [end]',\n",
              " '[start] Я був бідний. [end]',\n",
              " '[start] Він просто звичайний студент. [end]',\n",
              " '[start] Я хочу, щоб ти мені якомога швидше зателефонував. [end]',\n",
              " '[start] Минуло десять років із ти пір, коли ми бачилися востаннє. [end]',\n",
              " '[start] Том завтра не поїде до Бостона. [end]',\n",
              " '[start] Думаєш, Том би спробував зробити це? [end]',\n",
              " '[start] Том — геній. [end]',\n",
              " '[start] Том збоченець. [end]',\n",
              " '[start] Будеш трохи салату? [end]',\n",
              " '[start] Це я познайомив Тома з його дружиною. [end]',\n",
              " '[start] Я попрошу Тома почекати. [end]',\n",
              " '[start] Він гострив ніж. [end]',\n",
              " '[start] Я завжди прокидаюся о половині на сьому. [end]',\n",
              " '[start] Коли ми йдемо? [end]',\n",
              " '[start] Ти маєш залишатися тут, поки вони не повернуться. [end]',\n",
              " '[start] Куди Том поклав ключі? [end]',\n",
              " '[start] Ти маєш комусь довіряти. [end]',\n",
              " '[start] Що Том намагається зробити? [end]',\n",
              " '[start] Що ти плануєш робити влітку? [end]',\n",
              " '[start] До речі, де ви живете? [end]',\n",
              " '[start] Ми не завжди праві. [end]',\n",
              " '[start] Думка про відвідування церкви викликає у мене почуття незручності. [end]',\n",
              " '[start] Том вдома з батьками. [end]',\n",
              " '[start] Котра зараз година? [end]',\n",
              " '[start] Який твій улюблений гурт? [end]',\n",
              " '[start] Том спраглий. [end]',\n",
              " '[start] Мені здається, він сердитий. [end]',\n",
              " '[start] Ми з ними розірвали стосунки. [end]',\n",
              " '[start] Том думав, що Мері слухає. [end]',\n",
              " '[start] Не можу повірити, що людям подобається це сміття. [end]',\n",
              " '[start] Який у вас розмір ноги? [end]',\n",
              " '[start] Вона мала. [end]',\n",
              " '[start] Ти його знайшов. [end]',\n",
              " '[start] Ви знали, що Том помер в Бостоні? [end]',\n",
              " '[start] Том може лишитися. [end]',\n",
              " '[start] Радий познайомитися з вами, Томе. [end]',\n",
              " '[start] Скільки у вас грошей? [end]',\n",
              " '[start] Співай разом з нами. [end]',\n",
              " '[start] Всі розмовляли одночасно. [end]',\n",
              " '[start] Вона успадкувала блакитні очі від своєї матері. [end]',\n",
              " '[start] Їхні мрії збулися. [end]',\n",
              " '[start] Ви коли-небудь бачили кита? [end]',\n",
              " '[start] Це останній шанс. [end]']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = tf.keras.Sequential([Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.attention(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.dense_proj(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "JTfQHt0iYKf_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.attention1 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention2 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = tf.keras.Sequential([Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "        self.dropout3 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, enc_output, training):\n",
        "        attn_output1 = self.attention1(inputs, inputs)\n",
        "        attn_output1 = self.dropout1(attn_output1, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output1)\n",
        "        attn_output2 = self.attention2(out1, enc_output)\n",
        "        attn_output2 = self.dropout2(attn_output2, training=training)\n",
        "        out2 = self.layernorm2(out1 + attn_output2)\n",
        "        ffn_output = self.dense_proj(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        return self.layernorm3(out2 + ffn_output)\n"
      ],
      "metadata": {
        "id": "I0xtIhi3YOV2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = layers.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = Embedding(input_dim=vocab_size, output_dim=embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoderLayer(embed_dim, num_heads, latent_dim)(x, training=True)\n",
        "\n",
        "decoder_inputs = layers.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = layers.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = Embedding(input_dim=vocab_size, output_dim=embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoderLayer(embed_dim, num_heads, latent_dim)(x, encoder_outputs, training=True)\n",
        "decoder_outputs = Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "transformer = Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")\n",
        "\n",
        "\n",
        "transformer.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "def format_dataset(eng, ukr):\n",
        "    eng = eng_vectorization(eng)\n",
        "    ukr = ukr_vectorization(ukr)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": ukr[:, :-1]}, ukr[:, 1:])\n",
        "\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "def make_dataset(pairs, batch_size):\n",
        "    eng_texts, ukr_texts = zip(*pairs)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), list(ukr_texts)))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "train_ds = make_dataset(train_pairs, batch_size)\n",
        "val_ds = make_dataset(val_pairs, batch_size)\n",
        "\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haJ20sdEYZvw",
        "outputId": "e4248a46-3634-4a54-fad6-a3af7503d8bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "868/868 [==============================] - 4870s 6s/step - loss: 1.4914 - accuracy: 0.8074 - val_loss: 3.1767 - val_accuracy: 0.7121\n",
            "Epoch 2/3\n",
            "868/868 [==============================] - 4865s 6s/step - loss: 1.1184 - accuracy: 0.8466 - val_loss: 0.6312 - val_accuracy: 0.8908\n",
            "Epoch 3/3\n",
            "868/868 [==============================] - 4868s 6s/step - loss: 0.6907 - accuracy: 0.8894 - val_loss: 0.4837 - val_accuracy: 0.9088\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ef163d9cfa0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ukr_vocab = ukr_vectorization.get_vocabulary()\n",
        "ukr_index_lookup = dict(zip(range(len(ukr_vocab)), ukr_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = ukr_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = ukr_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence.replace(\"[start]\", \"\").strip()\n",
        "\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(5):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(f\"EN: {input_sentence}\\nUKR: {translated}\\n\")\n",
        "\n",
        "# Example translation\n",
        "print(\"Example translation of 'hi':\")\n",
        "print(decode_sequence('hi'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eULDEPwgpoBw",
        "outputId": "c7a0b4d5-a821-413d-a60f-895aadcd3cac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EN: Is this book yours?\n",
            "UKR: Так твоя книжка [end]\n",
            "\n",
            "EN: I'm surprised at your behavior.\n",
            "UKR: Боюся ваш улюблений напій [end]\n",
            "\n",
            "EN: Let him do it.\n",
            "UKR: Дайно ним [end]\n",
            "\n",
            "EN: Learning a foreign language is a waste of time.\n",
            "UKR: Час вивчати мови [end]\n",
            "\n",
            "EN: Nobody saw anything.\n",
            "UKR: Ніхто нічого бачив [end]\n",
            "\n",
            "Example translation of 'hi':\n",
            "Привіт Томе [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Проведіть експерименти з моделями бібліотеки Hugging Face (раніше - Hugging Face Transformers,) за допомогою (наприклад) Pipeline модуля**"
      ],
      "metadata": {
        "id": "i2wpPX4DGhc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbHI833sdhws",
        "outputId": "f2222963-8772-4e07-f9d7-e42578941da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcLyS9c-fyGQ",
        "outputId": "e28635e2-d965-4712-8b6d-76df7426789a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "uk_models = [\n",
        "    \"csebuetnlp/mT5_multilingual_XLSum\",\n",
        "    \"Geotrend/bert-base-uk-cased\",\n",
        "    \"IlyaGusev/rut5_base_sum_gazeta_uk\",\n",
        "    \"sberbank-ai/ruRoberta-large\"\n",
        "]\n",
        "\n",
        "# Створення пайплайнів\n",
        "pipelines = {}\n",
        "for model in uk_models:\n",
        "    try:\n",
        "        if \"sum\" in model:\n",
        "            pipelines[model] = pipeline(\"summarization\", model=model)\n",
        "        else:\n",
        "            pipelines[model] = pipeline(\"fill-mask\", model=model)\n",
        "    except Exception as e:\n",
        "        print(f\"Не вдалося створити пайплайн для моделі {model}: {e}\")\n",
        "\n",
        "# Приклад використання пайплайну для сумаризації\n",
        "text_for_summarization = \"Новий рік принесе щастя і мир\"\n",
        "try:\n",
        "    summarized_text = pipelines[\"csebuetnlp/mT5_multilingual_XLSum\"](text_for_summarization)\n",
        "    print(\"Сумаризований текст:\", summarized_text)\n",
        "except Exception as e:\n",
        "    print(f\"Помилка при сумаризації тексту: {e}\")\n",
        "\n",
        "# Приклад використання пайплайну для заповнення пропущених слів\n",
        "text_for_fill_mask = \"Вітаю з [MASK] роком\"\n",
        "try:\n",
        "    filled_text = pipelines[\"Geotrend/bert-base-uk-cased\"](text_for_fill_mask)\n",
        "    print(\"Текст із заповненими пропусками:\", filled_text)\n",
        "except Exception as e:\n",
        "    print(f\"Помилка при заповненні пропущених слів: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRYosZThdhrB",
        "outputId": "fee08228-76d1-45bc-d240-cb66dbb410cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Не вдалося створити пайплайн для моделі csebuetnlp/mT5_multilingual_XLSum: Couldn't instantiate the backend tokenizer from one of: \n",
            "(1) a `tokenizers` library serialization file, \n",
            "(2) a slow tokenizer instance to convert or \n",
            "(3) an equivalent slow tokenizer class to instantiate and convert. \n",
            "You need to have sentencepiece installed to convert a slow tokenizer to a fast one.\n",
            "Не вдалося створити пайплайн для моделі IlyaGusev/rut5_base_sum_gazeta_uk: IlyaGusev/rut5_base_sum_gazeta_uk is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
            "Помилка при сумаризації тексту: 'csebuetnlp/mT5_multilingual_XLSum'\n",
            "Текст із заповненими пропусками: [{'score': 0.26641497015953064, 'token': 10709, 'token_str': 'новим', 'sequence': 'Вітаю з новим роком'}, {'score': 0.2534734308719635, 'token': 8223, 'token_str': 'цим', 'sequence': 'Вітаю з цим роком'}, {'score': 0.09071118384599686, 'token': 6850, 'token_str': 'великим', 'sequence': 'Вітаю з великим роком'}, {'score': 0.053628019988536835, 'token': 6007, 'token_str': 'іншими', 'sequence': 'Вітаю з іншими роком'}, {'score': 0.048481278121471405, 'token': 3783, 'token_str': 'таким', 'sequence': 'Вітаю з таким роком'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Завдання щодо генерації або стилізації зображень (на вибір) Вирішіть завдання перенесення стилю або генерації зображень (архітектура за вашим вибором: GAN/DCGAN/VAE/Diffusion).**"
      ],
      "metadata": {
        "id": "g3nvg2hxGs97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9_OqEYpHuHD",
        "outputId": "90ddfe32-0263-4685-f09e-f31e75215879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "image_size = 28\n",
        "nz = 100\n",
        "ngf = 64\n",
        "ndf = 64\n",
        "num_epochs = 5\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(nz, ngf * 4, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # State size. (ngf*4) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # State size. (ngf*2) x 7 x 7\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # State size. (ngf) x 14 x 14\n",
        "            nn.ConvTranspose2d(ngf, 1, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # State size. (1) x 28 x 28\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Input size. (1) x 28 x 28\n",
        "            nn.Conv2d(1, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # State size. (ndf) x 14 x 14\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # State size. (ndf*2) x 7 x 7\n",
        "            nn.Conv2d(ndf * 2, 1, 7, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1, 1).squeeze(1)\n",
        "\n",
        "\n",
        "netG = Generator().to(device)\n",
        "netD = Discriminator().to(device)\n",
        "\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "\n",
        "        netD.zero_grad()\n",
        "        real_cpu = data[0].to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "        output = netD(real_cpu).view(-1)\n",
        "        errD_real = criterion(output, label)\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label).type(torch.float)\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "        errD_fake = criterion(output, label)\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "\n",
        "\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label).type(torch.float)\n",
        "        output = netD(fake).view(-1)\n",
        "        errG = criterion(output, label)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        optimizerG.step()\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (epoch % 1 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            save_image(fake, f'output_epoch_{epoch}.png', nrow=8, normalize=True)\n",
        "\n",
        "print(\"Training Finished.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Neiq2QTsIS5j",
        "outputId": "9e82e3a9-6739-40ea-a4a5-da88d74478db"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:03<00:00, 8.70MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 132kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 2.44MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 11.4MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "[0/5][0/469]\tLoss_D: 1.3299\tLoss_G: 1.1541\tD(x): 0.4626\tD(G(z)): 0.4194 / 0.3219\n",
            "[0/5][50/469]\tLoss_D: 0.0882\tLoss_G: 3.9089\tD(x): 0.9567\tD(G(z)): 0.0402 / 0.0211\n",
            "[0/5][100/469]\tLoss_D: 0.9872\tLoss_G: 3.9822\tD(x): 0.8342\tD(G(z)): 0.5208 / 0.0265\n",
            "[0/5][150/469]\tLoss_D: 0.8381\tLoss_G: 1.8900\tD(x): 0.7597\tD(G(z)): 0.4101 / 0.1657\n",
            "[0/5][200/469]\tLoss_D: 0.5141\tLoss_G: 2.3175\tD(x): 0.7942\tD(G(z)): 0.2346 / 0.1074\n",
            "[0/5][250/469]\tLoss_D: 0.6325\tLoss_G: 1.7137\tD(x): 0.6651\tD(G(z)): 0.1679 / 0.1936\n",
            "[0/5][300/469]\tLoss_D: 0.5901\tLoss_G: 1.6594\tD(x): 0.7411\tD(G(z)): 0.2281 / 0.2111\n",
            "[0/5][350/469]\tLoss_D: 0.7232\tLoss_G: 2.4543\tD(x): 0.8396\tD(G(z)): 0.4046 / 0.0994\n",
            "[0/5][400/469]\tLoss_D: 0.7390\tLoss_G: 1.0463\tD(x): 0.5994\tD(G(z)): 0.1546 / 0.3758\n",
            "[0/5][450/469]\tLoss_D: 0.7026\tLoss_G: 2.6981\tD(x): 0.8720\tD(G(z)): 0.4161 / 0.0815\n",
            "[1/5][0/469]\tLoss_D: 0.6530\tLoss_G: 1.9614\tD(x): 0.7967\tD(G(z)): 0.3307 / 0.1560\n",
            "[1/5][50/469]\tLoss_D: 0.7214\tLoss_G: 1.3840\tD(x): 0.6920\tD(G(z)): 0.2692 / 0.2660\n",
            "[1/5][100/469]\tLoss_D: 0.7083\tLoss_G: 1.6590\tD(x): 0.7452\tD(G(z)): 0.3154 / 0.2127\n",
            "[1/5][150/469]\tLoss_D: 0.7911\tLoss_G: 1.1077\tD(x): 0.5630\tD(G(z)): 0.1554 / 0.3607\n",
            "[1/5][200/469]\tLoss_D: 0.8619\tLoss_G: 1.4387\tD(x): 0.6778\tD(G(z)): 0.3403 / 0.2605\n",
            "[1/5][250/469]\tLoss_D: 0.9716\tLoss_G: 2.0265\tD(x): 0.8144\tD(G(z)): 0.5125 / 0.1482\n",
            "[1/5][300/469]\tLoss_D: 0.7827\tLoss_G: 1.6094\tD(x): 0.7153\tD(G(z)): 0.3361 / 0.2205\n",
            "[1/5][350/469]\tLoss_D: 0.8473\tLoss_G: 1.1198\tD(x): 0.6215\tD(G(z)): 0.2780 / 0.3464\n",
            "[1/5][400/469]\tLoss_D: 0.9128\tLoss_G: 1.0128\tD(x): 0.5257\tD(G(z)): 0.1904 / 0.3790\n",
            "[1/5][450/469]\tLoss_D: 1.0707\tLoss_G: 1.7440\tD(x): 0.7363\tD(G(z)): 0.5048 / 0.1918\n",
            "[2/5][0/469]\tLoss_D: 1.0425\tLoss_G: 1.8855\tD(x): 0.7755\tD(G(z)): 0.5203 / 0.1719\n",
            "[2/5][50/469]\tLoss_D: 0.9743\tLoss_G: 0.9784\tD(x): 0.5668\tD(G(z)): 0.2932 / 0.3972\n",
            "[2/5][100/469]\tLoss_D: 0.9507\tLoss_G: 0.9514\tD(x): 0.5254\tD(G(z)): 0.2161 / 0.4079\n",
            "[2/5][150/469]\tLoss_D: 0.9009\tLoss_G: 1.2339\tD(x): 0.6569\tD(G(z)): 0.3493 / 0.3132\n",
            "[2/5][200/469]\tLoss_D: 1.1006\tLoss_G: 0.7217\tD(x): 0.4408\tD(G(z)): 0.1898 / 0.5043\n",
            "[2/5][250/469]\tLoss_D: 0.9966\tLoss_G: 1.2158\tD(x): 0.6140\tD(G(z)): 0.3664 / 0.3156\n",
            "[2/5][300/469]\tLoss_D: 1.0068\tLoss_G: 0.9717\tD(x): 0.6130\tD(G(z)): 0.3792 / 0.3927\n",
            "[2/5][350/469]\tLoss_D: 1.0611\tLoss_G: 0.8629\tD(x): 0.5066\tD(G(z)): 0.2702 / 0.4388\n",
            "[2/5][400/469]\tLoss_D: 1.0808\tLoss_G: 1.4014\tD(x): 0.6884\tD(G(z)): 0.4692 / 0.2778\n",
            "[2/5][450/469]\tLoss_D: 1.0195\tLoss_G: 1.1458\tD(x): 0.6302\tD(G(z)): 0.3991 / 0.3365\n",
            "[3/5][0/469]\tLoss_D: 0.9951\tLoss_G: 1.1241\tD(x): 0.6102\tD(G(z)): 0.3666 / 0.3441\n",
            "[3/5][50/469]\tLoss_D: 1.2223\tLoss_G: 0.6035\tD(x): 0.4251\tD(G(z)): 0.2490 / 0.5664\n",
            "[3/5][100/469]\tLoss_D: 0.9888\tLoss_G: 1.3803\tD(x): 0.6549\tD(G(z)): 0.4074 / 0.2719\n",
            "[3/5][150/469]\tLoss_D: 1.0578\tLoss_G: 1.6788\tD(x): 0.7330\tD(G(z)): 0.4961 / 0.2052\n",
            "[3/5][200/469]\tLoss_D: 1.0703\tLoss_G: 1.1979\tD(x): 0.6299\tD(G(z)): 0.4213 / 0.3188\n",
            "[3/5][250/469]\tLoss_D: 1.2568\tLoss_G: 1.8931\tD(x): 0.7781\tD(G(z)): 0.6110 / 0.1685\n",
            "[3/5][300/469]\tLoss_D: 1.1767\tLoss_G: 0.8410\tD(x): 0.4916\tD(G(z)): 0.3382 / 0.4515\n",
            "[3/5][350/469]\tLoss_D: 1.0258\tLoss_G: 0.8460\tD(x): 0.5035\tD(G(z)): 0.2557 / 0.4441\n",
            "[3/5][400/469]\tLoss_D: 1.0808\tLoss_G: 0.9001\tD(x): 0.4856\tD(G(z)): 0.2582 / 0.4243\n",
            "[3/5][450/469]\tLoss_D: 1.0425\tLoss_G: 0.8730\tD(x): 0.5439\tD(G(z)): 0.3191 / 0.4321\n",
            "[4/5][0/469]\tLoss_D: 1.1281\tLoss_G: 1.1053\tD(x): 0.5841\tD(G(z)): 0.4083 / 0.3540\n",
            "[4/5][50/469]\tLoss_D: 1.1699\tLoss_G: 1.3088\tD(x): 0.7337\tD(G(z)): 0.5503 / 0.2932\n",
            "[4/5][100/469]\tLoss_D: 0.9946\tLoss_G: 1.1433\tD(x): 0.5847\tD(G(z)): 0.3380 / 0.3379\n",
            "[4/5][150/469]\tLoss_D: 1.1956\tLoss_G: 0.8106\tD(x): 0.5354\tD(G(z)): 0.3979 / 0.4616\n",
            "[4/5][200/469]\tLoss_D: 1.0523\tLoss_G: 1.2423\tD(x): 0.6663\tD(G(z)): 0.4519 / 0.3112\n",
            "[4/5][250/469]\tLoss_D: 1.1264\tLoss_G: 0.8159\tD(x): 0.5182\tD(G(z)): 0.3288 / 0.4593\n",
            "[4/5][300/469]\tLoss_D: 1.0662\tLoss_G: 1.0584\tD(x): 0.5860\tD(G(z)): 0.3792 / 0.3708\n",
            "[4/5][350/469]\tLoss_D: 1.2181\tLoss_G: 0.7504\tD(x): 0.4314\tD(G(z)): 0.2152 / 0.4914\n",
            "[4/5][400/469]\tLoss_D: 1.0406\tLoss_G: 1.1283\tD(x): 0.6310\tD(G(z)): 0.4120 / 0.3383\n",
            "[4/5][450/469]\tLoss_D: 1.0706\tLoss_G: 1.2355\tD(x): 0.6321\tD(G(z)): 0.4226 / 0.3102\n",
            "Training Finished.\n"
          ]
        }
      ]
    }
  ]
}